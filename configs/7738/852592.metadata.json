{
  "id": 852592,
  "name": "Test scores of the AI relative to human performance",
  "unit": "",
  "createdAt": "2024-04-02T22:09:24.000Z",
  "updatedAt": "2025-01-28T21:45:20.000Z",
  "coverage": "",
  "timespan": "1998-2023",
  "datasetId": 6457,
  "shortUnit": "",
  "columnOrder": 0,
  "shortName": "performance",
  "catalogPath": "grapher/artificial_intelligence/2024-04-02/dynabench/dynabench#performance",
  "descriptionShort": "Human performance, as the benchmark, is set to zero. The capability of each AI system is normalized to an initial performance of -100.",
  "descriptionProcessing": "We mapped the benchmarks to their respective domains based on a review of each benchmark's primary focus and the specific capabilities it tests within AI systems:\n\n- MNIST was mapped to \"Handwriting recognition\", as it tests AI systems' ability to recognize and classify handwritten digits, a fundamental task in the domain of digital image processing.\n- GLUE was categorized under \"Language understanding\" due to its assessment of models across a variety of linguistic tasks, highlighting the general capabilities of AI in understanding human language.\n- ImageNet was categorized as \"Image recognition\", focusing on the ability of AI systems to accurately identify and categorize images into predefined classes, showcasing the advancements in visual perception.\n- SQuAD 1.1 and SQuAD 2.0 were distinguished as \"Reading comprehension\" and \"Reading comprehension with unanswerable questions\" respectively. While both benchmarks evaluate reading comprehension, SQuAD 2.0 adds an extra layer of complexity with the introduction of unanswerable questions, demanding deeper understanding and reasoning from AI models.\n- BBH was aligned with \"Complex reasoning\", as it challenges AI with tasks that require not just logical reasoning but also creative thinking, simulating complex problem-solving scenarios.\n- Switchboard was associated with \"Speech recognition\" due to its focus on transcribing and understanding human speech within a conversational context, evaluating AI's ability to process and respond to spoken language.\n- MMLU was placed in \"General knowledge tests\", given its assessment across multiple disciplines and topics, requiring a broad and comprehensive understanding of language.\n- HellaSwag was mapped to \"Predictive reasoning\" for its evaluation of AI's ability to predict logical continuations within given contexts, testing commonsense reasoning and understanding.\n- HumanEval was categorized under \"Code generation\", focusing on AI's capability to understand programming languages and generate code that solves specific problems, highlighting skills in logical thinking and algorithmic problem-solving.\n- SuperGLUE was designated as \"Nuanced language interpretation\" due to its advanced set of linguistic tasks that require deep understanding, reasoning, and interpretation of text, pushing the boundaries of what AI can comprehend.\n- GSK8k was mapped to \"Math problem-solving\", as it tests AI on solving mathematical problems that involve reasoning and logical deduction, reflecting capabilities in numerical understanding and problem-solving.",
  "type": "float",
  "dataChecksum": "16035189526995955020",
  "metadataChecksum": "385759252263758656",
  "datasetName": "Dynabench: Rethinking Benchmarking in NLP",
  "updatePeriodDays": 365,
  "datasetVersion": "2024-04-02",
  "nonRedistributable": true,
  "display": {},
  "schemaVersion": 2,
  "processingLevel": "minor",
  "presentation": {
    "topicTagsLinks": [
      "Artificial Intelligence"
    ]
  },
  "dimensions": {
    "years": {
      "values": [
        {
          "id": 2021
        },
        {
          "id": 2022
        },
        {
          "id": 2023
        },
        {
          "id": 2020
        },
        {
          "id": 1998
        },
        {
          "id": 2002
        },
        {
          "id": 2003
        },
        {
          "id": 2006
        },
        {
          "id": 2010
        },
        {
          "id": 2012
        },
        {
          "id": 2013
        },
        {
          "id": 2018
        },
        {
          "id": 2009
        },
        {
          "id": 2014
        },
        {
          "id": 2015
        },
        {
          "id": 2016
        },
        {
          "id": 2019
        },
        {
          "id": 2017
        },
        {
          "id": 2011
        }
      ]
    },
    "entities": {
      "values": [
        {
          "id": 369314,
          "name": "Code generation",
          "code": null
        },
        {
          "id": 369321,
          "name": "Complex reasoning",
          "code": null
        },
        {
          "id": 369319,
          "name": "General knowledge tests",
          "code": null
        },
        {
          "id": 369315,
          "name": "Handwriting recognition",
          "code": null
        },
        {
          "id": 369313,
          "name": "Image recognition",
          "code": null
        },
        {
          "id": 369324,
          "name": "Language understanding",
          "code": null
        },
        {
          "id": 369316,
          "name": "Math problem-solving",
          "code": null
        },
        {
          "id": 369320,
          "name": "Nuanced language interpretation",
          "code": null
        },
        {
          "id": 369323,
          "name": "Predictive reasoning",
          "code": null
        },
        {
          "id": 369317,
          "name": "Reading comprehension",
          "code": null
        },
        {
          "id": 369318,
          "name": "Reading comprehension with unanswerable questions",
          "code": null
        },
        {
          "id": 369322,
          "name": "Speech recognition",
          "code": null
        }
      ]
    }
  },
  "origins": [
    {
      "id": 557,
      "title": "Dynabench: Rethinking Benchmarking in NLP",
      "description": "This dataset captures the progression of AI evaluation benchmarks, reflecting their adaptation to the rapid advancements in AI technology. The benchmarks cover a wide range of tasks, from language understanding to image processing, and are designed to test AI models' capabilities in various domains. The dataset includes performance metrics for each benchmark, providing insights into AI models' proficiency in different areas of machine learning research.\n\n  - BBH (BIG-Bench Hard): This benchmark serves as a rigorous evaluation framework for advanced language models, targeting their capacity for complex reasoning and problem-solving. It identifies tasks where AI models traditionally underperform compared to human benchmarks, emphasizing the enhancement of AI reasoning through innovative prompting methods like Chain-of-Thought.\n  - GLUE (General Language Understanding Evaluation): GLUE is a comprehensive benchmark suite designed to assess the breadth of an AI model's language understanding capabilities across a variety of tasks, including sentiment analysis, textual entailment, and question answering. It aims to advance the field towards more generalized models of language comprehension.\n  - GSM8K: This dataset challenges AI models with a collection of grade-level math word problems, designed to test computational and reasoning abilities. By requiring models to perform a sequence of arithmetic operations, GSM8K evaluates the AI's capacity for engaging in multi-step mathematical problem-solving.\n  - HellaSwag: HellaSwag assesses AI models on their ability to predict the continuation of scenarios, demanding a nuanced understanding of context and narrative. This benchmark pushes the boundaries of predictive modeling and contextual comprehension within AI systems.\n  - HumanEval: Targeting the intersection of AI and software development, HumanEval presents programming challenges to evaluate the code generation capabilities of AI models. This benchmark tests models' understanding of coding logic and their ability to produce functional code solutions.\n  - ImageNet: A cornerstone in the field of computer vision, ImageNet provides a large-scale dataset for object recognition and classification tasks. It benchmarks the ability of AI models to accurately identify and categorize images, serving as a foundational tool for visual AI research.\n  - MMLU (Massive Multitask Language Understanding): MMLU offers a diverse set of language understanding challenges, testing AI models across a broad spectrum of domains and task types. It aims to evaluate and promote the development of AI systems with comprehensive and adaptable language capabilities.\n  - MNIST: As a fundamental benchmark in image processing and computer vision, MNIST tests AI models on their ability to recognize handwritten digits. This dataset is pivotal in assessing the basic perceptual and pattern recognition capabilities of AI systems.\n  - SQuAD 1.1 and 2.0 (Stanford Question Answering Dataset): These benchmarks evaluate the reading comprehension abilities of AI models, requiring them to extract or infer answers from textual passages. SQuAD 2.0 further introduces the challenge of discerning unanswerable questions, adding a layer of complexity in judgment and inference.\n  - SuperGLUE: An extension of GLUE, SuperGLUE presents a set of more demanding language understanding tasks, designed to test the limits of AI models' reasoning, comprehension, and inference capabilities. It serves as a metric for cutting-edge advancements in natural language processing.\n  - Switchboard: This benchmark focuses on the processing and understanding of conversational speech, testing AI models on their ability to navigate the complexities of human dialogue. It highlights the challenges in speech recognition and natural language understanding within spontaneous communication.",
      "producer": "Kiela et al.",
      "citationFull": "Kiela, D., Thrush, T., Ethayarajh, K., & Singh, A. (2023) 'Plotting Progress in AI', Contextual AI Blog. Available at: https://contextual.ai/blog/plotting-progress (Accessed: 02 April 2024).",
      "urlMain": "https://contextual.ai/plotting-progress-in-ai/#contact",
      "dateAccessed": "2024-04-02",
      "datePublished": "2023",
      "license": {
        "name": "© 2024 Contextual AI"
      }
    }
  ]
}