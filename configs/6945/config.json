{
  "id": 6945,
  "map": {
    "columnSlug": "736819"
  },
  "data": {
    "availableEntities": [
      "6-layer MLP (MNIST)",
      "ADALINE",
      "ADAM (CIFAR-10)",
      "ALBERT-xxlarge",
      "ALVINN",
      "AlexNet",
      "AlexaTM 20B",
      "AlphaCode",
      "AlphaFold",
      "AlphaGo Fan",
      "AlphaGo Lee",
      "AlphaGo Master",
      "AlphaGo Zero",
      "AlphaStar",
      "AlphaZero",
      "AmoebaNet-A (F=448)",
      "BERT-Large",
      "Back-propagation",
      "BiLSTM for Speech",
      "BigGAN-deep 512x512",
      "Chinchilla",
      "ConSERT",
      "DALL-E",
      "DQN",
      "Decision tree (classification)",
      "Decoupled weight decay regularization",
      "DeepSpeech2 (English)",
      "Denoising Diffusion Probabilistic Models (LSUN Bedroom)",
      "Dropout (CIFAR)",
      "Dropout (ImageNet)",
      "Dropout (MNIST)",
      "EfficientNetV2",
      "Feedforward NN",
      "Fuzzy NN",
      "GANs",
      "GNMT",
      "GPT",
      "GPT-2",
      "GPT-3 175B (davinci)",
      "GPT-4",
      "GPT-NeoX-20B",
      "Gato",
      "GoogLeNet / InceptionV1",
      "Hanabi 4 player",
      "HuBERT",
      "Image Classification with the Fisher Vector: Theory and Practice",
      "Image generation",
      "Innervator",
      "JFT",
      "KN5 LM + RNN 400/10 (WSJ)",
      "KataGo",
      "LLaMA (65B)",
      "LSTM",
      "LaMDA",
      "LeNet-5",
      "Libratus",
      "M6-T",
      "MCDNN (MNIST)",
      "MSRA (C, PReLU)",
      "Meta Pseudo Labels",
      "Minerva (540B)",
      "Mitosis",
      "MnasNet-A1 + SSDLite",
      "MnasNet-A3",
      "MuZero",
      "NASv3 (CIFAR-10)",
      "NLLB",
      "NPLM",
      "Named Entity Recognition model",
      "Neocognitron",
      "NetTalk",
      "NÜWA",
      "ObjectNet",
      "OpenAI Five",
      "OpenAI Five Rerun",
      "PLUG",
      "PNASNet-5",
      "PaLM (540B)",
      "PaLM 2",
      "Pandemonium (morse)",
      "Part-of-sentence tagging model",
      "Parti",
      "Perceptron Mark I",
      "R-FCN",
      "RNN 500/10 + RT09 LM (NIST RT05)",
      "RNNsearch-50*",
      "ResNet-152 (ImageNet)",
      "SPPNet",
      "Samuel Neural Checkers",
      "Seq2Seq LSTM",
      "SmooCT",
      "Sparse all-MLP",
      "Stable Diffusion (LDM-KL-8-G)",
      "System 11",
      "T5-11B",
      "T5-3B",
      "TD-Gammon",
      "Taiyi-Stable Diffusion",
      "Theseus",
      "TransE",
      "Transformer",
      "Transformer local-attention (NesT-B)",
      "Unsupervised High-level Feature Learner",
      "VGG16",
      "ViT-H/14",
      "Visualizing CNNs",
      "Whisper",
      "Word2Vec (large)",
      "Xception",
      "YOLOv3",
      "Zip CNN",
      "wave2vec 2.0 LARGE",
      "ALBERT",
      "ASE+ACE",
      "AmoebaNet-A (F=190)",
      "BART-large",
      "BRNN",
      "CTC-Trained LSTM",
      "CURL",
      "CapsNet (MNIST)",
      "CapsNet (MultiMNIST)",
      "Codex",
      "CogVideo",
      "Constituency-Tree LSTM",
      "Context-dependent RNN",
      "DALL·E 2",
      "DBLSTM",
      "DImensionality Reduction",
      "DQN-2015",
      "Deep Belief Nets",
      "Deep Multitask NLP Network",
      "DenseNet-264",
      "Domain Adaptation",
      "DrLIM",
      "Dropout (TIMIT)",
      "ELMo",
      "EfficientNet-L2",
      "FLAN",
      "Flamingo",
      "Generative BST",
      "GloVe (32B)",
      "GloVe (6B)",
      "GroupLens",
      "Hiero",
      "Hopfield network",
      "IBM-5",
      "Inception v3",
      "Inception-ResNet-V2",
      "Inceptionv4",
      "Kohonen network",
      "LRCN",
      "LSTM with forget gates",
      "MEB",
      "MV-RNN",
      "MoCo",
      "MobileNet",
      "MobileNetV2",
      "Multiresolution CNN",
      "NASNet-A",
      "NLP from scratch",
      "PLATO-XL",
      "PNAS-net",
      "Pattern recognition and reading by machine",
      "Phrase-based translation",
      "PreTrans-3L-250H",
      "R-CNN (T-net)",
      "Rational DQN Average",
      "ReLU (NORB)",
      "ResNeXt-50",
      "ResNet-110 (CIFAR-10)",
      "ResNet-50 Billion-scale",
      "RetinaNet-R101",
      "RetinaNet-R50",
      "RoBERTa Large",
      "RoboCat",
      "Rotation",
      "SACHS",
      "SENet (ImageNet)",
      "SNARC",
      "Self Organizing System",
      "Semantic Hashing",
      "ShuffleNet v1",
      "ShuffleNet v2",
      "SimCLR",
      "SqueezeNet",
      "Support Vector Machines",
      "UL2",
      "Unsupervised Scale-Invariant Learning",
      "VGG19",
      "Word2Vec (small)",
      "XGLM",
      "XLMR-XXL",
      "XLNet",
      "YOLO",
      "YOLOv2",
      "data2vec (language)",
      "data2vec (speech)",
      "data2vec (vision)",
      "AT&T Labs and Rutgers University and Bell Communications Research",
      "AT&T Labs, Yahoo Research",
      "BPE",
      "BellKor 2009",
      "BigChaos 2008",
      "Brno University of Technology, Johns Hopkins University",
      "CNN Best Practices",
      "Google Inc",
      "Graz University of Technology, University of Oxford",
      "HMM Word Alignment",
      "IBM Model 4",
      "Inria Grenoble Rhône-Alpes",
      "InstructGPT",
      "MIT",
      "NEC Laboratories",
      "NYU",
      "New York University",
      "RWTH Aachen and University of Southern California",
      "Random Decision Forests",
      "Roke Manor Research",
      "Stanford University",
      "Thumbs Up?",
      "UC Davis, Cornell",
      "University of California San Diego & Shannon Laboratory, AT&T Labs",
      "University of California, San Diego",
      "University of Pennsylvania",
      "University of Rochester",
      "Warsaw University",
      "Word Representations"
    ]
  },
  "note": "Each domain's training data has a specific unit; for example, for vision it is images and for language it is words. This means systems can only be compared directly within the same domain.",
  "slug": "ai-training-computation-vs-parameters-by-domain",
  "type": "ScatterPlot",
  "title": "Training computation vs. parameters in notable AI systems, by domain",
  "xAxis": {
    "scaleType": "log",
    "canChangeScaleType": true
  },
  "yAxis": {
    "max": 100000000000,
    "min": 0,
    "scaleType": "log",
    "canChangeScaleType": true
  },
  "$schema": "https://files.ourworldindata.org/schemas/grapher-schema.003.json",
  "version": 14,
  "subtitle": "Computation is measured in total petaFLOP, which is 10¹⁵ [floating-point operations](#dod:flop). Parameters are variables in an AI system whose values are adjusted during training to establish how input data gets transformed into the desired output.",
  "originUrl": "https://ourworldindata.org/artificial-intelligence",
  "dimensions": [
    {
      "display": {
        "unit": "petaFLOP",
        "zeroDay": "1949-01-01",
        "yearIsDay": true,
        "includeInTable": true,
        "numDecimalPlaces": 0
      },
      "property": "y",
      "variableId": 736819
    },
    {
      "property": "x",
      "variableId": 736814
    },
    {
      "property": "size",
      "variableId": 736816
    },
    {
      "property": "color",
      "variableId": 736811
    }
  ],
  "entityType": "system",
  "isPublished": true,
  "entityTypePlural": "systems",
  "hideAnnotationFieldsInTitle": {
    "time": true
  }
}