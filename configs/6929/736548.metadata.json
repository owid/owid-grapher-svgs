{
  "id": 736548,
  "name": "Accuracy on Humanities knowledge tests - state of the art",
  "unit": "%",
  "description": "This benchmark assesses the accuracy of models in humanities knowledge based on the MMLU benchmark.\nThe MMLU benchmark covers a wide range of 57 subjects, including STEM, humanities, social sciences, and more. It encompasses subjects of varying difficulty levels, spanning from elementary concepts to advanced professional topics. This comprehensive benchmark assesses not only world knowledge but also problem-solving abilities.\n",
  "createdAt": "2023-07-03T14:54:56.000Z",
  "updatedAt": "2024-02-26T23:30:38.000Z",
  "coverage": "",
  "timespan": "",
  "datasetId": 6103,
  "shortUnit": "%",
  "columnOrder": 0,
  "shortName": "performance_humanities_state_of_the_art",
  "catalogPath": "grapher/artificial_intelligence/2023-06-14/papers_with_code_benchmarks_state_of_the_art/papers_with_code_benchmarks_state_of_the_art#performance_humanities_state_of_the_art",
  "datasetName": "Performance on Coding, Math, Language, Image Classification and Atari tasks - only state of the art(Papers With Code, 2023)",
  "datasetVersion": "2023-06-14",
  "type": "float",
  "nonRedistributable": false,
  "display": {
    "name": "Humanities",
    "unit": "%",
    "zeroDay": "2019-01-01",
    "shortUnit": "%",
    "yearIsDay": true,
    "numDecimalPlaces": 1
  },
  "schemaVersion": 1,
  "presentation": {
    "topicTagsLinks": [],
    "faqs": []
  },
  "descriptionKey": [],
  "source": {
    "id": 29583,
    "name": "Papers With Code (2023)",
    "dataPublishedBy": "Papers With Code",
    "dataPublisherSource": "",
    "link": "https://paperswithcode.com/",
    "retrievedDate": "2023-06-14",
    "additionalInfo": "\nThe goal of Papers With Code website is to compile a comprehensive collection of ML papers, code implementations, datasets, methods, and evaluation tables, all made freely available.\n\nThe comparisons to human performance are very approximate and based on small samples of people â€” they are only meant to give a rough comparison. You can read more details in the papers that describe the benchmarks:\n\n-Hendrycks et al (2021) Measuring Massive Multitask Language Understanding (MMLU) (page 3): https://arxiv.org/pdf/2009.03300.pdf\n\n-Hendrycks et al (2021) Measuring Mathematical Problem Solving With the MATH Dataset (page 5): https://arxiv.org/pdf/2103.03874v2.pdf\n"
  },
  "dimensions": {
    "years": {
      "values": [
        {
          "id": 44
        },
        {
          "id": 487
        },
        {
          "id": 513
        },
        {
          "id": 1072
        },
        {
          "id": 1183
        },
        {
          "id": 1190
        }
      ]
    },
    "entities": {
      "values": [
        {
          "id": 366954,
          "name": "State of the Art",
          "code": null
        }
      ]
    }
  },
  "origins": []
}