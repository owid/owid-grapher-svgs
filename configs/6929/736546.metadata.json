{
  "id": 736546,
  "name": "Coding performance on competitions - state of the art",
  "unit": "%",
  "description": "This benchmark measures the accuracy of models in coding competitions based on the APPS benchmark. The APPS benchmark focuses on coding ability and problem-solving in a natural language context. It aims to replicate the evaluation process used for human programmers by presenting coding problems in unrestricted natural language and assessing the correctness of solutions.\nThe coding tasks included in this benchmark are sourced from open-access coding websites such as Codeforces and Kattis. These tasks span a range of difficulty levels, from introductory to collegiate competition level. The benchmark evaluates the accuracy of models in solving programming tasks specifically designed for coding competitions.\n",
  "createdAt": "2023-07-03T14:54:56.000Z",
  "updatedAt": "2024-02-26T23:30:38.000Z",
  "coverage": "",
  "timespan": "",
  "datasetId": 6103,
  "shortUnit": "%",
  "columnOrder": 0,
  "shortName": "performance_code_any_competition_state_of_the_art",
  "catalogPath": "grapher/artificial_intelligence/2023-06-14/papers_with_code_benchmarks_state_of_the_art/papers_with_code_benchmarks_state_of_the_art#performance_code_any_competition_state_of_the_art",
  "datasetName": "Performance on Coding, Math, Language, Image Classification and Atari tasks - only state of the art(Papers With Code, 2023)",
  "datasetVersion": "2023-06-14",
  "type": "float",
  "nonRedistributable": false,
  "display": {
    "name": "Coding competitions",
    "unit": "%",
    "zeroDay": "2019-01-01",
    "shortUnit": "%",
    "yearIsDay": true,
    "numDecimalPlaces": 1
  },
  "schemaVersion": 1,
  "presentation": {
    "topicTagsLinks": [],
    "faqs": []
  },
  "descriptionKey": [],
  "source": {
    "id": 29583,
    "name": "Papers With Code (2023)",
    "dataPublishedBy": "Papers With Code",
    "dataPublisherSource": "",
    "link": "https://paperswithcode.com/",
    "retrievedDate": "2023-06-14",
    "additionalInfo": "\nThe goal of Papers With Code website is to compile a comprehensive collection of ML papers, code implementations, datasets, methods, and evaluation tables, all made freely available.\n\nThe comparisons to human performance are very approximate and based on small samples of people â€” they are only meant to give a rough comparison. You can read more details in the papers that describe the benchmarks:\n\n-Hendrycks et al (2021) Measuring Massive Multitask Language Understanding (MMLU) (page 3): https://arxiv.org/pdf/2009.03300.pdf\n\n-Hendrycks et al (2021) Measuring Mathematical Problem Solving With the MATH Dataset (page 5): https://arxiv.org/pdf/2103.03874v2.pdf\n"
  },
  "dimensions": {
    "years": {
      "values": [
        {
          "id": 870
        },
        {
          "id": 918
        },
        {
          "id": 1134
        },
        {
          "id": 1352
        },
        {
          "id": 1486
        }
      ]
    },
    "entities": {
      "values": [
        {
          "id": 366954,
          "name": "State of the Art",
          "code": null
        }
      ]
    }
  },
  "origins": []
}