{
  "id": 736547,
  "name": "Coding performance on interviews - state of the art",
  "unit": "%",
  "description": "This benchmark assesses the accuracy of models in coding interviews based on the APPS benchmark. The APPS benchmark focuses on coding ability and problem-solving in a natural language context, simulating the evaluation process employed during human programmer interviews. It presents coding problems in unrestricted natural language and evaluates the correctness of solutions.\nThe coding tasks within this benchmark are sourced from open-access coding websites such as Codeforces and Kattis. These tasks cover a spectrum of difficulty levels, ranging from introductory to collegiate competition level. The benchmark measures the accuracy of models in solving programming tasks specifically tailored for coding interviews.\n",
  "createdAt": "2023-07-03T14:54:56.000Z",
  "updatedAt": "2024-02-26T23:30:38.000Z",
  "coverage": "",
  "timespan": "",
  "datasetId": 6103,
  "shortUnit": "%",
  "columnOrder": 0,
  "shortName": "performance_code_any_interview_state_of_the_art",
  "catalogPath": "grapher/artificial_intelligence/2023-06-14/papers_with_code_benchmarks_state_of_the_art/papers_with_code_benchmarks_state_of_the_art#performance_code_any_interview_state_of_the_art",
  "datasetName": "Performance on Coding, Math, Language, Image Classification and Atari tasks - only state of the art(Papers With Code, 2023)",
  "datasetVersion": "2023-06-14",
  "type": "float",
  "nonRedistributable": false,
  "display": {
    "name": "Coding interviews",
    "unit": "%",
    "zeroDay": "2019-01-01",
    "shortUnit": "%",
    "yearIsDay": true,
    "numDecimalPlaces": 1
  },
  "schemaVersion": 1,
  "presentation": {
    "topicTagsLinks": [],
    "faqs": []
  },
  "descriptionKey": [],
  "source": {
    "id": 29583,
    "name": "Papers With Code (2023)",
    "dataPublishedBy": "Papers With Code",
    "dataPublisherSource": "",
    "link": "https://paperswithcode.com/",
    "retrievedDate": "2023-06-14",
    "additionalInfo": "\nThe goal of Papers With Code website is to compile a comprehensive collection of ML papers, code implementations, datasets, methods, and evaluation tables, all made freely available.\n\nThe comparisons to human performance are very approximate and based on small samples of people â€” they are only meant to give a rough comparison. You can read more details in the papers that describe the benchmarks:\n\n-Hendrycks et al (2021) Measuring Massive Multitask Language Understanding (MMLU) (page 3): https://arxiv.org/pdf/2009.03300.pdf\n\n-Hendrycks et al (2021) Measuring Mathematical Problem Solving With the MATH Dataset (page 5): https://arxiv.org/pdf/2103.03874v2.pdf\n"
  },
  "dimensions": {
    "years": {
      "values": [
        {
          "id": 870
        },
        {
          "id": 918
        },
        {
          "id": 1134
        },
        {
          "id": 1352
        },
        {
          "id": 1422
        }
      ]
    },
    "entities": {
      "values": [
        {
          "id": 366954,
          "name": "State of the Art",
          "code": null
        }
      ]
    }
  },
  "origins": []
}