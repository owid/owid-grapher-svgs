{
  "id": 5627,
  "map": {
    "columnSlug": "180076"
  },
  "data": {
    "availableEntities": [
      "Theseus",
      "Perceptron Mark I",
      "Pandemonium (morse)",
      "Samuel Neural Checkers",
      "ADALINE",
      "Neocognitron",
      "Back-propagation",
      "NetTalk",
      "NetTalk (dictionary)",
      "NetTalk (transcription)",
      "Innervator",
      "ALVINN",
      "Zip CNN",
      "TD-Gammon",
      "Fuzzy NN",
      "System 11",
      "LSTM",
      "LeNet-5",
      "Decision tree (classification)",
      "NPLM",
      "BiLSTM for Speech",
      "GPU DBNs",
      "6-layer MLP (MNIST)",
      "Feedforward NN",
      "KN5 LM + RNN 400/10 (WSJ)",
      "RNN 500/10 + RT09 LM (NIST RT05)",
      "MCDNN (MNIST)",
      "Dropout (CIFAR)",
      "Dropout (ImageNet)",
      "Dropout (MNIST)",
      "Unsupervised High-level Feature Learner",
      "AlexNet",
      "Image Classification with the Fisher Vector: Theory and Practice",
      "RNN+weight noise+dynamic eval",
      "Mitosis",
      "Word2Vec (large)",
      "Visualizing CNNs",
      "TransE",
      "DQN",
      "Image generation",
      "SPN-4+KN5",
      "GANs",
      "SPPNet",
      "SmooCT",
      "RNNsearch-50*",
      "VGG16",
      "Large regularized LSTM",
      "Seq2Seq LSTM",
      "ADAM (CIFAR-10)",
      "MSRA (C, PReLU)",
      "genCNN + dyn eval",
      "GoogLeNet / InceptionV1",
      "Search-Proven Best LSTM",
      "LSTM-Char-Large",
      "AlphaGo Fan",
      "DeepSpeech2 (English)",
      "ResNet-152 (ImageNet)",
      "Variational (untied weights, MC) LSTM (Large)",
      "AlphaGo Lee",
      "R-FCN",
      "VD-RHN",
      "Named Entity Recognition model",
      "Part-of-sentence tagging model",
      "GNMT",
      "Pointer Sentinel-LSTM (medium)",
      "Zoneout + Variational LSTM (WT2)",
      "Xception",
      "VD-LSTM+REAL Large",
      "NASv3 (CIFAR-10)",
      "Neural Architecture Search with base 8 and shared embeddings",
      "PolyNet",
      "AlphaGo Master",
      "Libratus",
      "DeepStack",
      "MoE",
      "Transformer",
      "JFT",
      "AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (WT2)",
      "RetinaNet-R101",
      "OpenAI TI7 DOTA 1v1",
      "EI-REHN-1000D",
      "GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (WT2)",
      "ISS",
      "AWD-LSTM+WT+Cache+IOG (WT2)",
      "AlphaGo Zero",
      "Fraternal dropout + AWD-LSTM 3-layer (WT2)",
      "AWD-LSTM-MoS + dynamic evaluation (WT2, 2017)",
      "PNASNet-5",
      "2-layer-LSTM+Deep-Gradient-Compression",
      "AlphaZero",
      "QRNN",
      "AmoebaNet-A (F=448)",
      "IMPALA",
      "ENAS",
      "4 layer QRNN (h=2500)",
      "LSTM (Hebbian, Cache, MbPA)",
      "YOLOv3",
      "ResNeXt-101 32x48d",
      "Dropout-LSTM+Noise(Bernoulli) (WT2)",
      "aLSTM(depth-2)+RecurrentPolicy (WT2)",
      "GPT",
      "DARTS",
      "Population-based DRL",
      "(ensemble): AWD-LSTM-DOC (fin) × 5 (WT2)",
      "LSTM+NeuralCache",
      "BigGAN-deep 512x512",
      "Transformer (Adaptive Input Embeddings)",
      "BERT-Large",
      "TrellisNet",
      "Fine-tuned-AWD-LSTM-DOC(fin)",
      "Multi-cell LSTM",
      "Decoupled weight decay regularization",
      "Transformer-XL Large",
      "Hanabi 4 player",
      "GPT-2 (1542M)",
      "ProxylessNAS",
      "KataGo",
      "FAIRSEQ Adaptive Inputs",
      "Cross-lingual alignment",
      "BERT-Large-CAS (PTB+WT2+WT103)",
      "AWD-LSTM-DRILL + dynamic evaluation† (WT2)",
      "MnasNet-A1 + SSDLite",
      "MnasNet-A3",
      "DLRM-2020",
      "Transformer-XL Large + Phrase Induction",
      "AWD-LSTM + MoS + Partial Shuffled",
      "Tensorized Transformer (257M)",
      "RoBERTa Large",
      "Pluribus",
      "ObjectNet",
      "Megatron-BERT",
      "Megatron-LM (8.3B)",
      "AlphaX-1",
      "DistilBERT",
      "T5-11B",
      "T5-3B",
      "AlphaStar",
      "Base LM + kNN LM + Continuous Cache",
      "Sandwich Transformer",
      "Noisy Student (L2)",
      "MuZero",
      "Transformer-XL DeFINE (141M)",
      "MMLSTM",
      "OpenAI Five",
      "OpenAI Five Rerun",
      "AlphaFold",
      "Meena",
      "TaLK Convolution",
      "ALBERT-xxlarge",
      "Turing-NLG",
      "Feedback Transformer",
      "TransformerXL + spectrum control",
      "Tensor-Transformer(1core)+PN (WT103)",
      "ELECTRA",
      "Once for All",
      "NAS+ESS (156M)",
      "GPT-3 175B (davinci)",
      "GShard (dense)",
      "DeLight",
      "ViT-H/14",
      "wave2vec 2.0 LARGE",
      "KEPLER",
      "CPM-Large",
      "CT-MoS (WT2)",
      "ERNIE-Doc (247M)",
      "CLIP (ViT L/14@336px)",
      "DALL-E",
      "SRU++ Large",
      "Meta Pseudo Labels",
      "M6-T",
      "PLUG",
      "ProtT5-XXL",
      "ConSERT",
      "CogView",
      "Transformer local-attention (NesT-B)",
      "ViT-G/14",
      "DeBERTa",
      "ALIGN",
      "Denoising Diffusion Probabilistic Models (LSUN Bedroom)",
      "EfficientNetV2",
      "Adaptive Input Transformer + RD",
      "ERNIE 3.0",
      "HuBERT",
      "FLAN",
      "PermuteFormer",
      "HyperClova",
      "Megatron-Turing NLG 530B",
      "Yuan 1.0",
      "base LM+GNN+kNN",
      "S4",
      "Florence",
      "NÜWA",
      "Gopher (280B)",
      "GLaM",
      "ERNIE 3.0 Titan",
      "AlphaCode",
      "GPT-NeoX-20B",
      "LaMDA",
      "Segatron-XL large, M=384 + HCP",
      "Chinchilla",
      "PaLM (540B)",
      "Stable Diffusion (LDM-KL-8-G)",
      "Sparse all-MLP",
      "Flamingo",
      "Gato",
      "Imagen",
      "Tranception",
      "DITTO",
      "Parti",
      "Minerva (540B)",
      "NLLB",
      "AlexaTM 20B",
      "GLM-130B",
      "Whisper",
      "Taiyi-Stable Diffusion",
      "Mogrifier RLSTM (WT2)",
      "BLOOM",
      "Galactica",
      "AR-LDM",
      "GPT-3.5 (text-davinci-003)",
      "Hybrid H3-2.7B",
      "Falcon-40B",
      "GPT-4",
      "PanGu-Σ",
      "PaLM 2",
      "Claude 2",
      "Llama 2",
      "Jais",
      "Swift",
      "Falcon 180B",
      "Ferret (13B)"
    ]
  },
  "note": "The Executive Order on AI refers to a directive issued by President Biden on October 30, 2023, aimed at establishing guidelines and standards for the responsible development and use of artificial intelligence within the United States.",
  "slug": "artificial-intelligence-training-computation-by-researcher-affiliation",
  "type": "ScatterPlot",
  "title": "Computation used to train notable AI systems, by affiliation of researchers",
  "xAxis": {
    "label": "Publication date"
  },
  "yAxis": {
    "max": 1000000000000,
    "scaleType": "log",
    "canChangeScaleType": true
  },
  "$schema": "https://files.ourworldindata.org/schemas/grapher-schema.003.json",
  "version": 174,
  "subtitle": "Computation is measured in total petaFLOP, which is 10¹⁵ [floating-point operations](#dod:flop) estimated from AI literature, albeit with some uncertainty. Estimates are expected to be accurate within a factor of 2, or a factor of 5 for recent undisclosed models like GPT-4.",
  "originUrl": "http://ourworldindata.org/artificial-intelligence",
  "colorScale": {
    "baseColorScheme": "owid-distinct",
    "binningStrategy": "manual",
    "legendDescription": "Affiliation",
    "customCategoryColors": {
      "No data": "#58ac8c",
      "Academia": "#1171b0",
      "Industry": "#ca011f",
      "Government": "#18470f",
      "Collaboration": "#7f7480",
      "Uncategorized": "#969696",
      "Research Collective": "#bc8e5a",
      "Research collective": "#bc8e5a",
      "Collaboration, Academia-leaning": "#8f94e3",
      "Collaboration, Industry-leaning": "#e66ca1",
      "Collaboration, majority industry": "#e66ca1"
    },
    "customCategoryLabels": {
      "games": "Games",
      "speech": "Speech",
      "vision": "Vision",
      "No data": "Other",
      "driving": "Driving",
      "general": "General",
      "language": "Language",
      "Research Collective": "Research collective",
      "Collaboration, Academia-leaning": "Collaboration, Academia-majority",
      "Collaboration, Industry-leaning": "Collaboration, Industry-majority"
    },
    "customNumericColorsActive": true
  },
  "dimensions": [
    {
      "display": {
        "name": "Training computation (petaFLOP)",
        "includeInTable": true
      },
      "property": "y",
      "variableId": 815786
    },
    {
      "property": "color",
      "variableId": 815781
    }
  ],
  "entityType": "system",
  "isPublished": true,
  "baseColorScheme": "owid-distinct",
  "comparisonLines": [
    {
      "label": "disclosure required at 100 billion petaFLOP under the Executive Order",
      "yEquals": "10^11"
    }
  ],
  "entityTypePlural": "systems",
  "hideConnectedScatterLines": false,
  "hideAnnotationFieldsInTitle": {
    "time": true,
    "entity": true,
    "changeInPrefix": true
  }
}