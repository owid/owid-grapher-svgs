{
  "id": 6947,
  "map": {
    "columnSlug": "736819"
  },
  "data": {
    "availableEntities": [
      "Theseus",
      "Perceptron Mark I",
      "Pandemonium (morse)",
      "Samuel Neural Checkers",
      "ADALINE",
      "Neocognitron",
      "Back-propagation",
      "NetTalk",
      "NetTalk (dictionary)",
      "NetTalk (transcription)",
      "Innervator",
      "ALVINN",
      "Zip CNN",
      "TD-Gammon",
      "Fuzzy NN",
      "System 11",
      "LSTM",
      "LeNet-5",
      "Decision tree (classification)",
      "NPLM",
      "BiLSTM for Speech",
      "GPU DBNs",
      "6-layer MLP (MNIST)",
      "Feedforward NN",
      "KN5 LM + RNN 400/10 (WSJ)",
      "RNN 500/10 + RT09 LM (NIST RT05)",
      "MCDNN (MNIST)",
      "Dropout (CIFAR)",
      "Dropout (ImageNet)",
      "Dropout (MNIST)",
      "Unsupervised High-level Feature Learner",
      "AlexNet",
      "Image Classification with the Fisher Vector: Theory and Practice",
      "RNN+weight noise+dynamic eval",
      "Mitosis",
      "Word2Vec (large)",
      "Visualizing CNNs",
      "TransE",
      "DQN",
      "Image generation",
      "SPN-4+KN5",
      "GANs",
      "SPPNet",
      "SmooCT",
      "RNNsearch-50*",
      "VGG16",
      "Large regularized LSTM",
      "Seq2Seq LSTM",
      "ADAM (CIFAR-10)",
      "MSRA (C, PReLU)",
      "genCNN + dyn eval",
      "GoogLeNet / InceptionV1",
      "Search-Proven Best LSTM",
      "LSTM-Char-Large",
      "AlphaGo Fan",
      "DeepSpeech2 (English)",
      "ResNet-152 (ImageNet)",
      "Variational (untied weights, MC) LSTM (Large)",
      "AlphaGo Lee",
      "R-FCN",
      "VD-RHN",
      "Named Entity Recognition model",
      "Part-of-sentence tagging model",
      "GNMT",
      "Pointer Sentinel-LSTM (medium)",
      "Zoneout + Variational LSTM (WT2)",
      "Xception",
      "VD-LSTM+REAL Large",
      "NASv3 (CIFAR-10)",
      "Neural Architecture Search with base 8 and shared embeddings",
      "PolyNet",
      "AlphaGo Master",
      "Libratus",
      "DeepStack",
      "MoE",
      "Transformer",
      "JFT",
      "AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (WT2)",
      "RetinaNet-R101",
      "OpenAI TI7 DOTA 1v1",
      "EI-REHN-1000D",
      "GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (WT2)",
      "ISS",
      "AWD-LSTM+WT+Cache+IOG (WT2)",
      "AlphaGo Zero",
      "Fraternal dropout + AWD-LSTM 3-layer (WT2)",
      "AWD-LSTM-MoS + dynamic evaluation (WT2, 2017)",
      "PNASNet-5",
      "2-layer-LSTM+Deep-Gradient-Compression",
      "AlphaZero",
      "QRNN",
      "AmoebaNet-A (F=448)",
      "IMPALA",
      "ENAS",
      "4 layer QRNN (h=2500)",
      "LSTM (Hebbian, Cache, MbPA)",
      "YOLOv3",
      "ResNeXt-101 32x48d",
      "Dropout-LSTM+Noise(Bernoulli) (WT2)",
      "aLSTM(depth-2)+RecurrentPolicy (WT2)",
      "GPT",
      "DARTS",
      "Population-based DRL",
      "(ensemble): AWD-LSTM-DOC (fin) × 5 (WT2)",
      "LSTM+NeuralCache",
      "BigGAN-deep 512x512",
      "Transformer (Adaptive Input Embeddings)",
      "BERT-Large",
      "TrellisNet",
      "Fine-tuned-AWD-LSTM-DOC(fin)",
      "Multi-cell LSTM",
      "Decoupled weight decay regularization",
      "Transformer-XL Large",
      "Hanabi 4 player",
      "GPT-2 (1542M)",
      "ProxylessNAS",
      "KataGo",
      "FAIRSEQ Adaptive Inputs",
      "Cross-lingual alignment",
      "BERT-Large-CAS (PTB+WT2+WT103)",
      "AWD-LSTM-DRILL + dynamic evaluation† (WT2)",
      "MnasNet-A1 + SSDLite",
      "MnasNet-A3",
      "DLRM-2020",
      "Transformer-XL Large + Phrase Induction",
      "AWD-LSTM + MoS + Partial Shuffled",
      "Tensorized Transformer (257M)",
      "RoBERTa Large",
      "Pluribus",
      "ObjectNet",
      "Megatron-BERT",
      "Megatron-LM (8.3B)",
      "AlphaX-1",
      "DistilBERT",
      "T5-11B",
      "T5-3B",
      "AlphaStar",
      "Base LM + kNN LM + Continuous Cache",
      "Sandwich Transformer",
      "Noisy Student (L2)",
      "MuZero",
      "Transformer-XL DeFINE (141M)",
      "MMLSTM",
      "OpenAI Five",
      "OpenAI Five Rerun",
      "AlphaFold",
      "Meena",
      "TaLK Convolution",
      "ALBERT-xxlarge",
      "Turing-NLG",
      "Feedback Transformer",
      "TransformerXL + spectrum control",
      "Tensor-Transformer(1core)+PN (WT103)",
      "ELECTRA",
      "Once for All",
      "NAS+ESS (156M)",
      "GPT-3 175B (davinci)",
      "GShard (dense)",
      "DeLight",
      "ViT-H/14",
      "wave2vec 2.0 LARGE",
      "KEPLER",
      "CPM-Large",
      "CT-MoS (WT2)",
      "ERNIE-Doc (247M)",
      "CLIP (ViT L/14@336px)",
      "DALL-E",
      "SRU++ Large",
      "Meta Pseudo Labels",
      "M6-T",
      "PLUG",
      "ProtT5-XXL",
      "ConSERT",
      "CogView",
      "Transformer local-attention (NesT-B)",
      "ViT-G/14",
      "DeBERTa",
      "ALIGN",
      "Denoising Diffusion Probabilistic Models (LSUN Bedroom)",
      "EfficientNetV2",
      "Adaptive Input Transformer + RD",
      "ERNIE 3.0",
      "HuBERT",
      "FLAN",
      "PermuteFormer",
      "HyperClova",
      "Megatron-Turing NLG 530B",
      "Yuan 1.0",
      "base LM+GNN+kNN",
      "S4",
      "Florence",
      "NÜWA",
      "Gopher (280B)",
      "GLaM",
      "ERNIE 3.0 Titan",
      "AlphaCode",
      "GPT-NeoX-20B",
      "LaMDA",
      "Segatron-XL large, M=384 + HCP",
      "Chinchilla",
      "PaLM (540B)",
      "Stable Diffusion (LDM-KL-8-G)",
      "Sparse all-MLP",
      "Flamingo",
      "Gato",
      "Imagen",
      "Tranception",
      "DITTO",
      "Parti",
      "Minerva (540B)",
      "NLLB",
      "AlexaTM 20B",
      "GLM-130B",
      "Whisper",
      "Taiyi-Stable Diffusion",
      "Mogrifier RLSTM (WT2)",
      "BLOOM",
      "Galactica",
      "AR-LDM",
      "GPT-3.5 (text-davinci-003)",
      "Hybrid H3-2.7B",
      "Falcon-40B",
      "GPT-4",
      "PanGu-Σ",
      "PaLM 2",
      "Claude 2",
      "Llama 2",
      "Jais",
      "Swift",
      "Falcon 180B",
      "Ferret (13B)",
      "SNARC",
      "Self Organizing System",
      "Pattern recognition and reading by machine",
      "Kohonen network",
      "Hopfield network",
      "ASE+ACE",
      "IBM-5",
      "GroupLens",
      "Support Vector Machines",
      "Bidirectional RNN",
      "LSTM with forget gates",
      "Phrase-based translation",
      "Unsupervised Scale-Invariant Learning",
      "SACHS",
      "Hiero",
      "DrLIM",
      "CTC-Trained LSTM",
      "DImensionality Reduction",
      "Deep Belief Nets",
      "Deep Multitask NLP Network",
      "Semantic Hashing",
      "ReLU (NORB)",
      "Domain Adaptation",
      "NLP from scratch",
      "Dropout (TIMIT)",
      "MV-RNN",
      "Context-dependent RNN",
      "LSTM-300units",
      "RNN+LDA+KN5+cache",
      "PreTrans-3L-250H",
      "Word2Vec (small)",
      "R-CNN (T-net)",
      "DBLSTM",
      "DOT(S)-RNN",
      "GloVe (32B)",
      "GloVe (6B)",
      "HyperNEAT",
      "Multiresolution CNN",
      "VGG19",
      "LRCN",
      "DQN-2015",
      "Constituency-Tree LSTM",
      "Trajectory-pooled conv nets",
      "YOLO",
      "Inception v3",
      "ResNet-110 (CIFAR-10)",
      "Inception-ResNet-V2",
      "Inceptionv4",
      "SqueezeNet",
      "DenseNet-264",
      "ResNet-1001",
      "ResNeXt-50",
      "YOLOv2",
      "MobileNet",
      "ShuffleNet v1",
      "AWD-LSTM",
      "NASNet-A",
      "RetinaNet-R50",
      "SENet (ImageNet)",
      "LSTM + dynamic eval",
      "CapsNet (MNIST)",
      "CapsNet (MultiMNIST)",
      "PNAS-net",
      "ELMo",
      "AmoebaNet-A (F=190)",
      "LSTM (2018)",
      "Rotation",
      "MobileNetV2",
      "ShuffleNet v2",
      "Big-Little Net",
      "AWD-LSTM-MoS+PDR + dynamic evaluation (WT2)",
      "AWD-LSTM-MoS + dynamic evaluation (WT2, 2018)",
      "GPipe (Amoeba)",
      "GPipe (Transformer)",
      "Transformer ELMo",
      "MT-DNN",
      "True-Regularization+Finetune+Dynamic-Eval",
      "Transformer-XL + RMS dynamic eval",
      "ResNeXt-101 Billion-scale",
      "ResNet-50 Billion-scale",
      "CPC v2",
      "EfficientNet-L2",
      "XLM",
      "XLNet",
      "AMDIM",
      "FixRes ResNeXt-101 WSL",
      "BigBiGAN",
      "EN^2AS with performance reward",
      "Mogrifier (d2, MoS2, MC) + dynamic eval",
      "Adaptive Inputs + LayerDrop",
      "ALBERT",
      "BART-large",
      "MoCo",
      "Big Transfer (BiT-L)",
      "Theseus 6/768",
      "SimCLR",
      "Temporal Convolutional Attention-based Network(TCAN) (WT2)",
      "Routing Transformer",
      "CURL",
      "EfficientDet",
      "ERNIE-GEN (large)",
      "ViT-Base/32",
      "ViT-Huge/14",
      "SimCLRv2",
      "CLIP (ResNet-50)",
      "BigSSL",
      "Rational DQN Average",
      "Generative BST",
      "Megatron-LM (1T)",
      "Codex",
      "Zidong Taichu",
      "XLMR-XXL",
      "MEB",
      "PLATO-XL",
      "XGLM",
      "ERNIE-ViLG",
      "data2vec (language)",
      "data2vec (speech)",
      "data2vec (vision)",
      "RETRO-7B",
      "DeepNet",
      "DALL·E 2",
      "UL2",
      "CogVideo",
      "Phenaki",
      "Vicuna-13B",
      "RoboCat",
      "Stable Diffusion XL",
      "InternLM",
      "Robot Parkour",
      "Motion-Driven 3D Feature Tracking",
      "SRN-Encoded Grammatical Structures",
      "Iterative Bootstrapping WSD",
      "Random Decision Forests",
      "HMM Word Alignment",
      "SVM for face detection",
      "Sparse coding model for V1 receptive fields",
      "Social and content-based classification",
      "IBM Model 4",
      "Perceptron for Large Margin Classification",
      "FrameNet role labeling",
      "Thumbs Up?",
      "Maximum Entropy Models for machine translation",
      "CNN Best Practices",
      "Max-Margin Markov Networks",
      "ConvNet similarity metric",
      "Histograms of Oriented Gradients",
      "Sparse Energy-Based Model",
      "Restricted Bolzmann machines",
      "Regularized SVD for Collaborative Filtering",
      "BigChaos 2008",
      "BellKor 2008",
      "BellKor 2009",
      "BigChaos OptiBlend",
      "MatrixFac for Recommenders",
      "BellKor 2007",
      "Word Representations",
      "YouTube Video Recommendation System",
      "RNN-SpeedUp",
      "DSN",
      "Deeply-supervised nets",
      "BPE",
      "Spatiotemporal fusion ConvNet",
      "Fisher Kernel GMM",
      "InstructGPT",
      "Statement Curriculum Learning"
    ]
  },
  "note": "Each domain's training data has a specific unit; for example, for vision it is images and for language it is words. This means systems can only be compared directly within the same domain.",
  "slug": "ai-training-computation-vs-parameters-by-researcher-affiliation",
  "type": "ScatterPlot",
  "title": "Training computation vs. parameters in notable AI systems, by researcher affiliation",
  "xAxis": {
    "scaleType": "log",
    "canChangeScaleType": true
  },
  "yAxis": {
    "max": 100000000000,
    "min": 0,
    "scaleType": "log",
    "canChangeScaleType": true
  },
  "$schema": "https://files.ourworldindata.org/schemas/grapher-schema.003.json",
  "version": 22,
  "subtitle": "Computation is measured in total petaFLOP, which is 10¹⁵ [floating-point operations](#dod:flop) estimated from AI literature, albeit with some uncertainty. Parameters are variables in an AI system whose values are adjusted during training to establish how input data gets transformed into the desired output.",
  "originUrl": "https://ourworldindata.org/artificial-intelligence",
  "colorScale": {
    "customCategoryColors": {
      "No data": "#58ac8c",
      "Academia": "#1171b0",
      "Industry": "#ca011f",
      "Government": "#18470f",
      "Collaboration": "#7f7480",
      "Research collective": "#bc8e5a",
      "Collaboration, majority academia": "#8f94e3",
      "Collaboration, majority industry": "#e66ca1"
    },
    "customNumericColorsActive": true
  },
  "dimensions": [
    {
      "display": {
        "name": "Training computation (petaFLOP)",
        "includeInTable": true
      },
      "property": "y",
      "variableId": 815786
    },
    {
      "property": "x",
      "variableId": 815783
    },
    {
      "property": "size",
      "variableId": 815784
    },
    {
      "property": "color",
      "variableId": 815781
    }
  ],
  "entityType": "system",
  "isPublished": true,
  "entityTypePlural": "systems",
  "hideAnnotationFieldsInTitle": {
    "time": true
  }
}