{
  "id": 987260,
  "name": "Annual number of large-scale AI systems by country",
  "unit": "AI systems",
  "createdAt": "2024-09-10T10:27:11.000Z",
  "updatedAt": "2024-09-10T10:27:11.000Z",
  "coverage": "",
  "timespan": "2017-2024",
  "datasetId": 6706,
  "shortUnit": "",
  "columnOrder": 0,
  "shortName": "yearly_count",
  "catalogPath": "grapher/artificial_intelligence/2024-09-09/epoch_compute_intensive_countries/epoch_compute_intensive_countries#yearly_count",
  "descriptionShort": "Refers to the location of the primary organization with which the authors of a large-scale AI systems are affiliated. The 2024 data is incomplete and was last updated 10 September 2024.",
  "descriptionProcessing": "The number of large-scale AI systems by country is determined by tallying the number of machine learning models that are associated with the geographical location of the researchers' affiliated institutions. It's important to note that a single model can have multiple authors, each potentially affiliated with different institutions, thus contributing to the count for multiple countries.",
  "type": "int",
  "grapherConfigIdETL": "0191db79-17a9-70e2-a3ba-5a0c685c2601",
  "datasetName": "Large-scale AI systems by country",
  "updatePeriodDays": 31,
  "datasetVersion": "2024-09-09",
  "nonRedistributable": false,
  "display": {
    "unit": "AI systems"
  },
  "schemaVersion": 2,
  "processingLevel": "major",
  "presentation": {
    "topicTagsLinks": [
      "Artificial Intelligence"
    ],
    "faqs": []
  },
  "descriptionKey": [],
  "dimensions": {
    "years": {
      "values": [
        {
          "id": 2017
        },
        {
          "id": 2020
        },
        {
          "id": 2021
        },
        {
          "id": 2022
        },
        {
          "id": 2023
        },
        {
          "id": 2024
        }
      ]
    },
    "entities": {
      "values": [
        {
          "id": 1,
          "name": "United Kingdom",
          "code": "GBR"
        },
        {
          "id": 3,
          "name": "France",
          "code": "FRA"
        },
        {
          "id": 6,
          "name": "Germany",
          "code": "DEU"
        },
        {
          "id": 12,
          "name": "Russia",
          "code": "RUS"
        },
        {
          "id": 13,
          "name": "United States",
          "code": "USA"
        },
        {
          "id": 14,
          "name": "Japan",
          "code": "JPN"
        },
        {
          "id": 44,
          "name": "Canada",
          "code": "CAN"
        },
        {
          "id": 72,
          "name": "United Arab Emirates",
          "code": "ARE"
        },
        {
          "id": 86,
          "name": "Singapore",
          "code": "SGP"
        },
        {
          "id": 127,
          "name": "South Korea",
          "code": "KOR"
        },
        {
          "id": 133,
          "name": "Israel",
          "code": "ISR"
        },
        {
          "id": 144,
          "name": "Hong Kong",
          "code": "HKG"
        },
        {
          "id": 155,
          "name": "Finland",
          "code": "FIN"
        },
        {
          "id": 171,
          "name": "China",
          "code": "CHN"
        },
        {
          "id": 34678,
          "name": "Total",
          "code": null
        },
        {
          "id": 369566,
          "name": "Multinational",
          "code": null
        }
      ]
    }
  },
  "origins": [
    {
      "id": 1224,
      "title": "Tracking Compute-Intensive AI Models",
      "description": "A dataset that tracks compute-intensive AI models, with training compute over 10²³ floating point operations (FLOP). This corresponds to training costs of hundreds of thousands of dollars or more. \n\nTo identify compute-intensive AI models, the team at Epoch AI used various resources, estimating compute when not directly reported. They included benchmarks and repositories, such as Papers With Code and Hugging Face, to find models exceeding 10²³ FLOP. They also explored non-English media and specific leaderboards, particularly focusing on Chinese sources.\n\nAdditionally, they examined blog posts, press releases from major labs, and scholarly literature to track new models. A separate table was created for models with unconfirmed but plausible compute levels. Despite thorough methods, proprietary and secretive models may have been missed.",
      "producer": "Epoch",
      "citationFull": "Robi Rahman, David Owen and Josh You (2024), \"Tracking Compute-Intensive AI Models\". Published online at epochai.org. Retrieved from: 'https://epochai.org/blog/tracking-compute-intensive-ai-models' [online resource]",
      "urlMain": "https://epochai.org/blog/tracking-compute-intensive-ai-models",
      "urlDownload": "https://epochai.org/data/epochdb/large_scale_ai_models.csv",
      "dateAccessed": "2024-08-05",
      "datePublished": "2024-06-19",
      "license": {
        "url": "https://epochai.org/blog/how-much-does-it-cost-to-train-frontier-ai-models",
        "name": "CC BY 4.0"
      }
    }
  ]
}