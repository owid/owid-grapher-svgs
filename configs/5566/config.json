{
  "id": 5566,
  "map": {
    "columnSlug": "180076"
  },
  "data": {
    "availableEntities": [
      "Theseus",
      "Perceptron Mark I",
      "Pandemonium (morse)",
      "Samuel Neural Checkers",
      "ADALINE",
      "Neocognitron",
      "Back-propagation",
      "NetTalk",
      "NetTalk (dictionary)",
      "NetTalk (transcription)",
      "Innervator",
      "ALVINN",
      "Zip CNN",
      "TD-Gammon",
      "Fuzzy NN",
      "System 11",
      "LSTM",
      "LeNet-5",
      "Decision tree (classification)",
      "NPLM",
      "BiLSTM for Speech",
      "GPU DBNs",
      "6-layer MLP (MNIST)",
      "Feedforward NN",
      "KN5 LM + RNN 400/10 (WSJ)",
      "RNN 500/10 + RT09 LM (NIST RT05)",
      "MCDNN (MNIST)",
      "Dropout (CIFAR)",
      "Dropout (ImageNet)",
      "Dropout (MNIST)",
      "Unsupervised High-level Feature Learner",
      "AlexNet",
      "Image Classification with the Fisher Vector: Theory and Practice",
      "RNN+weight noise+dynamic eval",
      "Mitosis",
      "Word2Vec (large)",
      "Visualizing CNNs",
      "TransE",
      "DQN",
      "Image generation",
      "SPN-4+KN5",
      "GANs",
      "SPPNet",
      "SmooCT",
      "RNNsearch-50*",
      "VGG16",
      "Large regularized LSTM",
      "Seq2Seq LSTM",
      "Fractional Max-Pooling",
      "ADAM (CIFAR-10)",
      "MSRA (C, PReLU)",
      "genCNN + dyn eval",
      "GoogLeNet / InceptionV1",
      "Search-Proven Best LSTM",
      "LSTM-Char-Large",
      "AlphaGo Fan",
      "DeepSpeech2 (English)",
      "ResNet-152 (ImageNet)",
      "Variational (untied weights, MC) LSTM (Large)",
      "AlphaGo Lee",
      "Named Entity Recognition model",
      "Part-of-sentence tagging model",
      "R-FCN",
      "VD-RHN",
      "GNMT",
      "Pointer Sentinel-LSTM (medium)",
      "Zoneout + Variational LSTM (WT2)",
      "Xception",
      "VD-LSTM+REAL Large",
      "NASv3 (CIFAR-10)",
      "Neural Architecture Search with base 8 and shared embeddings",
      "PolyNet",
      "AlphaGo Master",
      "Libratus",
      "DeepStack",
      "MoE",
      "Transformer",
      "JFT",
      "AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (WT2)",
      "RetinaNet-R101",
      "OpenAI TI7 DOTA 1v1",
      "EI-REHN-1000D",
      "GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (WT2)",
      "ISS",
      "AWD-LSTM+WT+Cache+IOG (WT2)",
      "AlphaGo Zero",
      "Fraternal dropout + AWD-LSTM 3-layer (WT2)",
      "AWD-LSTM-MoS + dynamic evaluation (WT2, 2017)",
      "PNASNet-5",
      "2-layer-LSTM+Deep-Gradient-Compression",
      "AlphaZero",
      "QRNN",
      "AmoebaNet-A (F=448)",
      "IMPALA",
      "ENAS",
      "4 layer QRNN (h=2500)",
      "LSTM (Hebbian, Cache, MbPA)",
      "YOLOv3",
      "ResNeXt-101 32x48d",
      "Dropout-LSTM+Noise(Bernoulli) (WT2)",
      "aLSTM(depth-2)+RecurrentPolicy (WT2)",
      "GPT",
      "DARTS",
      "FTW",
      "Population-based DRL",
      "(ensemble): AWD-LSTM-DOC (fin) × 5 (WT2)",
      "LSTM+NeuralCache",
      "BigGAN-deep 512x512",
      "Transformer (Adaptive Input Embeddings)",
      "BERT-Large",
      "TrellisNet",
      "Fine-tuned-AWD-LSTM-DOC(fin)",
      "Multi-cell LSTM",
      "Decoupled weight decay regularization",
      "Transformer-XL Large",
      "Hanabi 4 player",
      "GPT-2 (1542M)",
      "ProxylessNAS",
      "KataGo",
      "FAIRSEQ Adaptive Inputs",
      "Cross-lingual alignment",
      "BERT-Large-CAS (PTB+WT2+WT103)",
      "AWD-LSTM-DRILL + dynamic evaluation† (WT2)",
      "MnasNet-A1 + SSDLite",
      "MnasNet-A3",
      "DLRM-2020",
      "Transformer-XL Large + Phrase Induction",
      "AWD-LSTM + MoS + Partial Shuffled",
      "Tensorized Transformer (257M)",
      "RoBERTa Large",
      "Pluribus",
      "ObjectNet",
      "Megatron-BERT",
      "Megatron-LM (8.3B)",
      "AlphaX-1",
      "DistilBERT",
      "T5-11B",
      "T5-3B",
      "AlphaStar",
      "Base LM + kNN LM + Continuous Cache",
      "Sandwich Transformer",
      "Noisy Student (L2)",
      "MuZero",
      "Transformer-XL DeFINE (141M)",
      "MMLSTM",
      "OpenAI Five",
      "OpenAI Five Rerun",
      "DD-PPO",
      "AlphaFold",
      "ContextNet + Noisy Student",
      "Meena",
      "TaLK Convolution",
      "ALBERT-xxlarge",
      "Turing-NLG",
      "Feedback Transformer",
      "TransformerXL + spectrum control",
      "Tensor-Transformer(1core)+PN (WT103)",
      "ELECTRA",
      "Once for All",
      "NAS+ESS (156M)",
      "DETR",
      "GPT-3 175B (davinci)",
      "iGPT-XL",
      "GShard (dense)",
      "DeLight",
      "ProBERTa",
      "Conformer + Wav2vec 2.0 + Noisy Student ",
      "mT5-XXL",
      "ViT-Huge/14",
      "wave2vec 2.0 LARGE",
      "KEPLER",
      "AlphaFold 2",
      "CPM-Large",
      "CT-MoS (WT2)",
      "ERNIE-Doc (247M)",
      "CLIP (ViT L/14@336px)",
      "DALL-E",
      "Switch",
      "MSA Transformer",
      "SRU++ Large",
      "Meta Pseudo Labels",
      "M6-T",
      "PLUG",
      "ProtT5-XXL",
      "ADM",
      "ConSERT",
      "CogView",
      "Transformer local-attention (NesT-B)",
      "ByT5-XXL",
      "ViT-G/14",
      "CoAtNet",
      "EMDR",
      "DeBERTa",
      "ALIGN",
      "Denoising Diffusion Probabilistic Models (LSUN Bedroom)",
      "EfficientNetV2",
      "Adaptive Input Transformer + RD",
      "ERNIE 3.0",
      "GOAT",
      "HuBERT",
      "SEER",
      "DNABERT",
      "FLAN 137B",
      "PermuteFormer",
      "HyperClova",
      "AlphaFold-Multimer",
      "Megatron-Turing NLG 530B",
      "Yuan 1.0",
      "base LM+GNN+kNN",
      "S4",
      "CodeT5-base",
      "BASIC-L",
      "Florence",
      "NÜWA",
      "Gopher (280B)",
      "GLaM",
      "ERNIE 3.0 Titan",
      "AlphaCode",
      "GPT-NeoX-20B",
      "LaMDA",
      "ProteinBERT",
      "PolyCoder",
      "ViT-G (model soup)",
      "Segatron-XL large, M=384 + HCP",
      "Chinchilla",
      "PaLM (540B)",
      "BERT-RBP",
      "Stable Diffusion (LDM-KL-8-G)",
      "Sparse all-MLP",
      "Flamingo",
      "OPT-175B",
      "UL2",
      "Gato",
      "Imagen",
      "Tranception",
      "DITTO",
      "CoCa",
      "Parti",
      "ProGen2",
      "Minerva (540B)",
      "CodeT5-large",
      "NLLB",
      "AlexaTM 20B",
      "GLM-130B",
      "BlenderBot 3",
      "PaLI",
      "Whisper",
      "U-PaLM",
      "U-PaLM (540B)",
      "ESM-2",
      "Taiyi-Stable Diffusion",
      "Mogrifier RLSTM (WT2)",
      "BLOOM",
      "BLOOM-176B",
      "Galactica",
      "Fusion in Encoder",
      "AR-LDM",
      "Discriminator Guidance",
      "GPT-3.5 (text-davinci-003)",
      "CaLM",
      "Hybrid H3-2.7B",
      "Ankh",
      "Ankh_base",
      "Ankh_large",
      "DDPM-IP (CelebA)",
      "LLaMA-65B",
      "LLaMA-7B",
      "DiT-XL/2",
      "Falcon-40B",
      "GPT-4",
      "PanGu-Σ",
      "BloombergGPT",
      "Incoder-6.7B",
      "WizardLM-7B",
      "StarCoder",
      "PaLM 2",
      "ONE-PEACE",
      "Claude 2",
      "Llama 2",
      "Llama 2-70B",
      "Llama 2-7B",
      "Jais",
      "Swift",
      "Falcon 180B",
      "XGen-7B",
      "CODEFUSION (Python)",
      "ChatGLM3",
      "Skywork-13B",
      "Yi-34B",
      "Inflection-2",
      "Qwen-72B",
      "Gemini Ultra",
      "SNARC",
      "Genetic algorithm",
      "Self Organizing System",
      "Sequence-based pattern recognition",
      "Conditional probability machines",
      "Pattern recognition and reading by machine",
      "LMS",
      "Heuristic problem solving for AI",
      "MADALINE I",
      "BOXES",
      "GLEE",
      "Graph-based structural reasoning",
      "Naive Bayes",
      "Cognitron",
      "TD(0)",
      "Internal functionality of visual invariants",
      "Kohonen network",
      "Hopfield network",
      "ASE+ACE",
      "Learnability theory of language development",
      "Error Propagation",
      "PDP model for serial order",
      "Optimized Multi-Scale Edge Detection",
      "Motion-Driven 3D Feature Tracking",
      "Q-learning",
      "Time-delay neural networks",
      "Universal approximation via Feedforward Networks",
      "MADALINE III",
      "MLP as Bayesian Approximator",
      "SRN-Encoded Grammatical Structures",
      "REINFORCE in Stochastic Connectionism",
      "IBM-5",
      "GroupLens",
      "Iterative Bootstrapping WSD",
      "Random Decision Forests",
      "Support Vector Machines",
      "HMM Word Alignment",
      "SVM for face detection",
      "Bidirectional RNN",
      "Sparse coding model for V1 receptive fields",
      "Social and content-based classification",
      "LSTM with forget gates",
      "IBM Model 4",
      "Perceptron for Large Margin Classification",
      "SVD in recommender systems",
      "FrameNet role labeling",
      "Immediate trihead",
      "Gradient Boosting Machine",
      "Thumbs Up?",
      "NEAT in neuroevolution",
      "Tagging via Viterbi Decoding",
      "Maximum Entropy Models for machine translation",
      "Statistical Shape Constellations",
      "LDA",
      "Phrase-based translation",
      "Unsupervised Scale-Invariant Learning",
      "CNN Best Practices",
      "Max-Margin Markov Networks",
      "SACHS",
      "Hiero",
      "ConvNet similarity metric",
      "Histograms of Oriented Gradients",
      "Stanley (DARPA Grand Challenge 2)",
      "TFE SVM",
      "FAST",
      "DrLIM",
      "Spatial Pyramid Matching",
      "CTC-Trained LSTM",
      "DImensionality Reduction",
      "Deep Belief Nets",
      "Local Binary Patterns for facial recognition",
      "Greedy layer-wise DNN training",
      "Sparse Energy-Based Model",
      "λ-WASP",
      "Restricted Bolzmann machines",
      "Regularized SVD for Collaborative Filtering",
      "BLSTM for handwriting (1)",
      "Enhanced Neighborhood-Based Filtering",
      "BLSTM for handwriting (2)",
      "Multiscale deformable part model",
      "Deep Multitask NLP Network",
      "Denoising Autoencoders",
      "Semi-Supervised Embedding for DL",
      "Boss (DARPA Urban Challenge)",
      "Sparse digit recognition SVM",
      "BigChaos 2008",
      "Semantic Hashing",
      "RBM Image Classifier",
      "Deep Boltzmann Machines",
      "Conv-DBN",
      "BellKor 2008",
      "BellKor 2009",
      "BigChaos OptiBlend",
      "Pragmatic Theory solution (Netflix 2009)",
      "MatrixFac for Recommenders",
      "BellKor 2007",
      "3D city reconstruction",
      "Learning deep architectures",
      "Stacked Denoising Autoencoders",
      "Word Representations",
      "Deconvolutional Network",
      "Mid-level Features",
      "ReLU (LFW)",
      "ReLU (NORB)",
      "RBM-tuning",
      "Fisher-Boost",
      "YouTube Video Recommendation System",
      "Culturome",
      "Optimized Single-layer Net",
      "Deep rectifier networks",
      "RNN-SpeedUp",
      "Cross-Lingual POS Tagger",
      "Recursive sentiment autoencoder",
      "Adaptive Subgrad",
      "Domain Adaptation",
      "NLP from scratch",
      "HOGWILD!",
      "Dropout (TIMIT)",
      "MV-RNN",
      "Context-dependent RNN",
      "LSTM-300units",
      "RNN+LDA+KN5+cache",
      "Bayesian automated hyperparameter tuning",
      "Textual Imager",
      "Maxout Networks",
      "PreTrans-3L-250H",
      "SearchFusion",
      "SemVec",
      "Word2Vec (small)",
      "R-CNN (T-net)",
      "TensorReasoner",
      "DBLSTM",
      "Network in Network",
      "DOT(S)-RNN",
      "OverFeat",
      "GloVe (32B)",
      "GloVe (6B)",
      "HyperNEAT",
      "GRUs",
      "Two-stream ConvNets for action recognition",
      "RNN-WER",
      "DeepFace",
      "Multiresolution CNN",
      "VGG19",
      "DSN",
      "Deeply-supervised nets",
      "Spatially-Sparse CNN",
      "LRCN",
      "Fully Convolutional Networks",
      "Cascaded LNet-ANet",
      "NTM",
      "DeepLab",
      "CRF-RNN",
      "DQN-2015",
      "Constituency-Tree LSTM",
      "Fast R-CNN",
      "Deep LSTM for video classification",
      "Trajectory-pooled conv nets",
      "Faster R-CNN",
      "YOLO",
      "BatchNorm",
      "Listen, Attend and Spell",
      "BPE",
      "Deep Deterministic Policy Gradients",
      "Multi-scale Dilated CNN",
      "Netflix Recommender System",
      "Inception v3",
      "ResNet-110 (CIFAR-10)",
      "BPL",
      "Advantage Learning",
      "Convolutional Pose Machines",
      "A3C FF hs",
      "Inception-ResNet-V2",
      "Inceptionv4",
      "SqueezeNet",
      "Binarized Neural Network (MNIST)",
      "Symmetric Residual Encoder-Decoder Net",
      "Gated HORNN (3rd order)",
      "Spatiotemporal fusion ConvNet",
      "DMN",
      "Wide & Deep",
      "fastText",
      "Character-enriched word2vec",
      "SimpleNet",
      "DenseNet-264",
      "Multi-task Cascaded CNN",
      "WaveNet",
      "Youtube recommendation model",
      "MS-CNN",
      "ResNet-1001",
      "ResNet-200",
      "Stacked hourglass network",
      "TSN",
      "Wide Residual Network",
      "Differentiable neural computer",
      "Deeply-recursive ConvNet",
      "ResNeXt-50",
      "RefineNet",
      "Image-to-image cGAN",
      "Elastic weight consolidation",
      "PointNet",
      "GAN-Advancer",
      "Diabetic Retinopathy Detection Net",
      "GCNN-14",
      "YOLOv2",
      "Fisher Kernel GMM",
      "OR-WideResNet",
      "DnCNN",
      "Prototypical networks",
      "Mask R-CNN",
      "WGAN-GP",
      "MobileNet",
      "DeepLab (2017)",
      "SRGAN",
      "Inflated 3D ConvNet",
      "PointNet++",
      "EDSR",
      "HRA",
      "DeepLabV3",
      "NoisyNet-Dueling",
      "ShuffleNet v1",
      "AWD-LSTM",
      "NASNet-A",
      "PSPNet",
      "RetinaNet-R50",
      "Cutout-regularized net",
      "NeuMF (Pinterest)",
      "SENet (ImageNet)",
      "PyramidNet",
      "LSTM + dynamic eval",
      "LRSO-GAN",
      "CapsNet (MNIST)",
      "CapsNet (MultiMNIST)",
      "ProgressiveGAN",
      "S-Norm",
      "TriNet",
      "PNAS-net",
      "Refined Part Pooling",
      "ULM-FiT",
      "ELMo",
      "AmoebaNet-A (F=190)",
      "DeepLabV3+",
      "TCN (P-MNIST)",
      "Spectrally Normalized GAN",
      "Residual Dense Network",
      "Chinese - English translation",
      "LSTM (2018)",
      "Rotation",
      "Relational Memory Core",
      "MobileNetV2",
      "ShuffleNet v2",
      "RCAN",
      "Big-Little Net",
      "AWD-LSTM-MoS+PDR + dynamic evaluation (WT2)",
      "ESRGAN",
      "AWD-LSTM-MoS + dynamic evaluation (WT2, 2018)",
      "MetaMimic",
      "MemoReader",
      "GPipe (Amoeba)",
      "GPipe (Transformer)",
      "Transformer ELMo",
      "MT-DNN",
      "True-Regularization+Finetune+Dynamic-Eval",
      "WeNet (Penn Treebank)",
      "Transformer-XL + RMS dynamic eval",
      "SpecAugment",
      "DANet",
      "ResNeXt-101 Billion-scale",
      "ResNet-50 Billion-scale",
      "CPC v2",
      "EfficientNet-L2",
      "XLM",
      "XLNet",
      "AMDIM",
      "Char-CNN-BiLSTM",
      "FixRes ResNeXt-101 WSL",
      "PG-SWGAN",
      "LaNet-L (CIFAR-10)",
      "Walking Minotaur robot",
      "BigBiGAN",
      "EN^2AS with performance reward",
      "Mogrifier (d2, MoS2, MC) + dynamic eval",
      "Adaptive Inputs + LayerDrop",
      "ALBERT",
      "BART-large",
      "MoCo",
      "Transformer - LibriVox + Decoding/Rescoring",
      "Photo-Geometric Autoencoder",
      "StarGAN v2",
      "Big Transfer (BiT-L)",
      "Theseus 6/768",
      "Perceiver IO",
      "SimCLR",
      "Temporal Convolutional Attention-based Network(TCAN) (WT2)",
      "Routing Transformer",
      "MetNet",
      "Agent57",
      "CURL",
      "Go-explore",
      "UnifiedQA",
      "ContextNet",
      "Conformer",
      "Retrieval-Augmented Generator",
      "SemExp",
      "Hopfield Networks (2020)",
      "EfficientDet",
      "ERNIE-GEN (large)",
      "ViT-Base/32",
      "SimCLRv2",
      "ESM-1",
      "VQGAN + CLIP",
      "CLIP (ResNet-50)",
      "BigSSL",
      "top-down frozen classifier",
      "Rational DQN Average",
      "Generative BST",
      "Codex",
      "6-Act Tether",
      "W2v-BERT",
      "Zidong Taichu",
      "XLMR-XXL",
      "MEB",
      "PLATO-XL",
      "EfficientZero",
      "Projected GAN",
      "ViT-G/14 (LiT)",
      "DeBERTaV3-large + KEAR ",
      "Player of Games",
      "Contriever",
      "GLIDE",
      "LDM-1.45B",
      "XGLM",
      "ERNIE-ViLG",
      "data2vec (language)",
      "data2vec (speech)",
      "data2vec (vision)",
      "AbLang",
      "InstructGPT",
      "RETRO-7B",
      "Midjourney V1",
      "ST-MoE",
      "DeepNet",
      "Statement Curriculum Learning",
      "DALL·E 2",
      "SimCSE",
      "CogVideo",
      "Diffusion-GAN",
      "MetaLM",
      "Make-A-Video",
      "Phenaki",
      "AltCLIP",
      "CICERO",
      "ALM 1.0",
      "DiT-XL/2 + Discriminator Guidance",
      "ChatGPT (gpt-3.5-turbo)",
      "Gen-1",
      "BASIC-L + Lion",
      "PaLM-E",
      "Claude",
      "Vicuna-13B",
      "Agile Soccer Robot",
      "ImageBind",
      "CodeT5+",
      "Goat-7B",
      "PaLI-X",
      "MusicGen",
      "RoboCat",
      "ERNIE 3.5",
      "Stable Diffusion XL",
      "InternLM",
      "Robot Parkour",
      "AlphaMissense",
      "GPT-4V",
      "Show-1",
      "CTM (CIFAR-10)",
      "FinGPT-13B",
      "Ferret (13B)",
      "DALL·E 3",
      "DiT-XL/2 + CADS",
      "Cohere Embed",
      "BLUUMI",
      "Grok-1",
      "OmniVec",
      "Claude 2.1"
    ]
  },
  "logo": "owid",
  "note": "The Executive Order on AI refers to a directive issued by President Biden on October 30, 2023, aimed at establishing guidelines and standards for the responsible development and use of artificial intelligence within the United States.",
  "slug": "artificial-intelligence-training-computation",
  "type": "ScatterPlot",
  "title": "Computation used to train notable artificial intelligence systems",
  "xAxis": {
    "label": "Publication date"
  },
  "yAxis": {
    "max": 500000000000,
    "scaleType": "log",
    "canChangeScaleType": true
  },
  "$schema": "https://files.ourworldindata.org/schemas/grapher-schema.003.json",
  "version": 151,
  "subtitle": "Computation is measured in total petaFLOP, which is 10¹⁵ [floating-point operations](#dod:flop) estimated from AI literature, albeit with some uncertainty. Estimates are expected to be accurate within a factor of 2, or a factor of 5 for recent undisclosed models like GPT-4.",
  "originUrl": "https://ourworldindata.org/artificial-intelligence",
  "colorScale": {
    "baseColorScheme": "owid-distinct",
    "binningStrategy": "manual",
    "legendDescription": "Task domain",
    "customCategoryLabels": {
      "games": "Games",
      "speech": "Speech",
      "vision": "Vision",
      "No data": "Other",
      "driving": "Driving",
      "general": "General",
      "language": "Language"
    }
  },
  "dimensions": [
    {
      "display": {
        "name": "Training computation (petaFLOP)",
        "unit": "petaFLOP",
        "includeInTable": true
      },
      "property": "y",
      "variableId": 815786
    },
    {
      "property": "color",
      "variableId": 815780
    }
  ],
  "entityType": "system",
  "isPublished": true,
  "comparisonLines": [
    {
      "label": "disclosure required at 100 billion petaFLOP under the Executive Order",
      "yEquals": "10^11"
    }
  ],
  "entityTypePlural": "systems",
  "hideConnectedScatterLines": false,
  "hideAnnotationFieldsInTitle": {
    "time": true,
    "entity": true,
    "changeInPrefix": true
  }
}